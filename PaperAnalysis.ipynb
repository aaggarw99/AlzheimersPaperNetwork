{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alzheimer's Paper Network Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we look at several analysis techniques for a subset of publications in the PubMed Database (~16k papers). After natural language processing, the papers are categorized to 75 topics given their keywords -- and if they do not have keywords, we produce them using their abstracts + a ngram student_t distribution. Furthermore, the goal is to conduce page rank tests on the data and eventually produce future ranks for papers in the alzheimer's field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "from database import *\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import collections\n",
    "from stemming.porter2 import stem\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.collocations import TrigramAssocMeasures, TrigramCollocationFinder\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define's PaperAnalysis Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperAnalysis():\n",
    "    \"\"\"\n",
    "    This class holds most of the analysis and accessor methods for the project. In addition, the data we're using is stored in the class variable 'data'\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        df = pd.read_csv(\"pubmedID_min5clusters_v2.csv\", names=['PubID', 'CusterNo'])\n",
    "        # gets the ID's of the papers already clustered using SpeakEasy\n",
    "        self.min5_ids = df.PubID\n",
    "        if not os.path.exists('paper_data.pickle'):\n",
    "            # Further reduces data to only stems of trees in the network (meaning, degree = 0)\n",
    "            self.data = self.get_valid_set()\n",
    "            pickle.dump(self.data, open('paper_data.pickle', 'wb'))\n",
    "        else:\n",
    "            with open('paper_data.pickle', 'rb') as f:\n",
    "                with db_session:\n",
    "                    self.data = pickle.load(f)\n",
    "\n",
    "    def get_valid_set(self):\n",
    "        \"\"\"\n",
    "        Reduces down overall data to papers that are stems (meaning, they are the source of network trees)\n",
    "        \"\"\"\n",
    "        paper_list_id = []\n",
    "        paper_list = []\n",
    "        with db_session:\n",
    "            for idx in self.min5_ids:\n",
    "                if Papers.get(id=np.asscalar(idx)):\n",
    "                    if Papers.get(id=np.asscalar(idx)).degree == 0:\n",
    "                        paper_list.append(Papers.get(id=np.asscalar(idx)))\n",
    "        return paper_list\n",
    "    \n",
    "    def get_paper_from_id(self, idx, with_a_k=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        -----\n",
    "        idx = id of paper\n",
    "        with_a_k = when set to true, will only return paper information if it has an abstract and keywords\n",
    "\n",
    "        Return:\n",
    "        --------\n",
    "        Paper object attributes in the following format:\n",
    "        Paper properties: [id, title, abstract, keywords, year, month]\n",
    "\n",
    "        \"\"\"\n",
    "        with db_session:\n",
    "            if with_a_k==False:\n",
    "                if Papers.get(id=idx):\n",
    "                    if Papers.get(id=idx).abstract != None and Papers.get(id=idx).keywords != None:\n",
    "                        return Papers.get(id=idx)\n",
    "                print(\"No paper with an abstract or keywords was found\")\n",
    "                return []\n",
    "            else:\n",
    "                if Papers.get(id=idx):\n",
    "                    return Papers.get(id=idx)\n",
    "                print(\"No paper was found\")\n",
    "                return []\n",
    "                \n",
    "\n",
    "                \n",
    "    def get_keywords(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        -----\n",
    "        idx : id of the paper (-1 indicates the entire database)\n",
    "        \n",
    "        Return:\n",
    "        -------\n",
    "        Return list of keywords\n",
    "        \"\"\"\n",
    "        all_keywords = []\n",
    "        with db_session:\n",
    "            # look at the entire database keywords\n",
    "            if idx == -1:\n",
    "                for k in (select(p.keywords for p in Papers if p.keywords != None)):\n",
    "                    all_keywords += k.split(',')\n",
    "\n",
    "                return all_keywords\n",
    "            if Papers.get(id=idx):\n",
    "                \n",
    "                keyword_str = Papers.get(id=idx).keywords\n",
    "                if keyword_str == None:\n",
    "                    return []\n",
    "                all_keywords += keyword_str.split(\",\")\n",
    "                return all_keywords\n",
    "            return []\n",
    "\n",
    "    def get_title(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        -----\n",
    "        idx : id of the paper (-1 indicates the entire database)\n",
    "        \n",
    "        Return:\n",
    "        -------\n",
    "        A list of words in the title of paper idx\n",
    "        \"\"\"\n",
    "        all_title_words = []\n",
    "\n",
    "        with db_session:\n",
    "\n",
    "            if idx == -1:\n",
    "                all_titles = select(p.title for p in Papers if p.title != None or p.title!=\"\")[:]\n",
    "                for t in all_titles:\n",
    "                    all_title_words += t.split(' ')\n",
    "\n",
    "                return all_title_words\n",
    "\n",
    "            if Papers.get(id=idx):\n",
    "                title_str = Papers.get(id=idx).title\n",
    "                if title_str == None:\n",
    "                    return []\n",
    "                all_title_words += title_str.split(\" \")\n",
    "                return all_title_words\n",
    "            return []\n",
    "            \n",
    "    def get_abstract(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        -----\n",
    "        idx : id of the paper\n",
    "        \n",
    "        Return:\n",
    "        -------\n",
    "        Return a list of words in the title of paper idx\n",
    "        \"\"\"\n",
    "        all_abstract_words = []\n",
    "\n",
    "        with db_session:\n",
    "            if idx == -1:\n",
    "                all_abstract = select(p.abstract for p in Papers if p.abstract != None and p.abstract!=\"\")[:]\n",
    "                for a in all_abstract:\n",
    "                    all_abstract_words += a.split(' ')\n",
    "                return all_abstract_words\n",
    "\n",
    "            if Papers.get(id=idx):\n",
    "                abstract_str = Papers.get(id=idx).abstract\n",
    "                if abstract_str == None:\n",
    "                    return []\n",
    "                all_abstract_words += abstract_str.split(\" \")\n",
    "                return all_abstract_words\n",
    "\n",
    "            return []\n",
    "\n",
    "    def get_citations(self, idx):\n",
    "        with db_session:\n",
    "            return Citations.get(paper=idx).cited_by\n",
    "                \n",
    "    def keyword_in_abstract(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        -----\n",
    "        idx: paperID\n",
    "\n",
    "        Return:\n",
    "        -------\n",
    "        Percentage of how often keywords appear in the abstract OR -1 if the paper has either no keywords or abstract\n",
    "        \"\"\"\n",
    "\n",
    "        # removes duplicates in keywords list and abstract words list\n",
    "\n",
    "        unique_k = list(set(self.nlp(self.get_keywords(idx))))\n",
    "        split_k = []\n",
    "        for i in unique_k:\n",
    "            split_k += i.split(\" \")\n",
    "        if not unique_k:\n",
    "            return -1\n",
    "        unique_a = list(set(self.nlp(self.get_abstract(idx))))\n",
    "        if not unique_a:\n",
    "            return -1\n",
    "        in_abstract = 0\n",
    "        for i in split_k:\n",
    "            if i in unique_a:\n",
    "                in_abstract += 1\n",
    "                continue\n",
    "\n",
    "        return (in_abstract / len(split_k))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Natural Word Processing\n",
    "    def nlp(self, words):\n",
    "        \"\"\"\n",
    "        Performs natural language processing on a list of words\n",
    "\n",
    "        Args:\n",
    "        -----\n",
    "        words : list of words\n",
    "\n",
    "        Return:\n",
    "        -------\n",
    "        List of filtered words\n",
    "        \"\"\"\n",
    "        filtered = []\n",
    "\n",
    "        stopWords = set(stopwords.words('english'))\n",
    "        words = [w.lower() for w in words if w.lower() not in stopWords]\n",
    "        words = [w for w in words if re.match(\"[a-zA-Z]{2,}\", w)]\n",
    "        words = [re.sub(r'[^\\w\\s]','',w) for w in words]\n",
    "        for w in words:\n",
    "            x = w\n",
    "            x = x.replace(\"â€™\", \"'\")\n",
    "            x = x.replace(\"'s\", \"\")\n",
    "            x = x.replace(\":\", \"\")\n",
    "            x = x.replace(\"Î±\", \"\")\n",
    "            x = x.replace('Î²', '')\n",
    "            if nltk.stem.WordNetLemmatizer().lemmatize(x, 'v') == x:\n",
    "                x = nltk.stem.WordNetLemmatizer().lemmatize(x, 'n')\n",
    "            else:\n",
    "                x = nltk.stem.WordNetLemmatizer().lemmatize(x, 'v')\n",
    "            filtered.append(x)\n",
    "        return filtered\n",
    "\n",
    "    # finds most common element in a list\n",
    "    def most_common(self, lst, ct=5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        -----\n",
    "        lst : list of items\n",
    "        ct : integer\n",
    "        Specifies to print the top \"ct\" items\n",
    "        \n",
    "        Return:\n",
    "        -------\n",
    "        The most common items in a list\n",
    "        \"\"\"\n",
    "        counter = collections.Counter(lst)\n",
    "        return counter.most_common(ct)\n",
    "\n",
    "    def show_wordcloud(self, lst, subset=\"all\", feature=\"keywords\"):\n",
    "        \"\"\"\n",
    "        Displays a wordcloud\n",
    "\n",
    "        Args:\n",
    "        -----\n",
    "        lst : list of words\n",
    "        subset : specify \"all\" or cluster_number for the visual\n",
    "        feature : specify the type of Paper attribute for the visual\n",
    "        \"\"\"\n",
    "\n",
    "        all_string = ' '.join(map(str, lst))\n",
    "        wordcloud = WordCloud(background_color='white',\n",
    "                              width=2400,\n",
    "                              height=1500\n",
    "                              ).generate(all_string)\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.axis('off')\n",
    "        if subset == \"all\":\n",
    "            plt.title(\"Most common \"+feature+\" in the entire database\")\n",
    "        else:\n",
    "            plt.title(\"Most common \"+feature+\" in cluster: \"+subset)\n",
    "        plt.show()\n",
    "\n",
    "    def ngram_analyze(self, lst, model=\"student_t\"):\n",
    "        \"\"\"\n",
    "        Documentation for analysis tools:\n",
    "        http://www.nltk.org/_modules/nltk/metrics/association.html\n",
    "\n",
    "        Uses student_t distribution to analyze a list of words by splitting them into \\\n",
    "        tuples of 3 elements: eg. (a, b, c), (b, c, d), ...\n",
    "\n",
    "        The distribution assigns a score to each tuple. This function returns the \\\n",
    "        highest score words\n",
    "\n",
    "        Args:\n",
    "        -----\n",
    "        lst : a list of words\n",
    "        model : the chosen model for ngram analysis (student_t, chi_sq, mi_like, pmi, jaccard)\n",
    "        \n",
    "        Return:\n",
    "        -------\n",
    "        List of the top 9 words\n",
    "        \"\"\"\n",
    "        lst = self.nlp(lst)\n",
    "        string = \" \".join(map(str, lst))\n",
    "        words = nltk.word_tokenize(string)\n",
    "\n",
    "        measures = TrigramAssocMeasures()\n",
    "\n",
    "        finder = TrigramCollocationFinder.from_words(words)\n",
    "\n",
    "        scores = []\n",
    "\n",
    "        if model == \"student_t\":\n",
    "            scores = finder.score_ngrams(measures.student_t)[:]\n",
    "        elif model == \"chi_sq\":\n",
    "            scores = finder.score_ngrams(measures.chi_sq)[:]\n",
    "        elif model == \"mi_like\":\n",
    "            scores = finder.score_ngrams(measures.mi_like)[:]\n",
    "        elif model == \"pmi\":\n",
    "            scores = finder.score_ngrams(measures.pmi)[:]\n",
    "        elif model == \"jaccard\":\n",
    "            scores = finder.score_ngrams(measures.jaccard)[:]\n",
    "        else:\n",
    "            print(\"Not valid model!\")\n",
    "\n",
    "        scores.sort(key=lambda i:i[1], reverse=True)\n",
    "        top = scores[:3]\n",
    "        return top\n",
    "    # LDA model\n",
    "\n",
    "    def categorize(self):\n",
    "        \"\"\"\n",
    "        Obtain counter of keywords\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        Counter with all keywords that show up at least more than 5 times and less than 1000 times.\n",
    "        \"\"\"\n",
    "        cnt = Counter()\n",
    "        for paper in self.data:\n",
    "            key_singular = parse_keywords(paper)\n",
    "            keywords = self.nlp(key_singular) \n",
    "            for i in keywords:\n",
    "                cnt[i] +=1\n",
    "                \n",
    "        from itertools import dropwhile\n",
    "        # quality control\n",
    "        for key, count in dropwhile(lambda key_count: key_count[1] > 5, cnt.most_common()):\n",
    "            del cnt[key]\n",
    "        del cnt['alzheimers']\n",
    "        del cnt['alzheimer']\n",
    "        del cnt['disease']\n",
    "        del cnt['dementia']\n",
    "        del cnt['amyloid']\n",
    "        del cnt['cognitive']\n",
    "        del cnt['protein']\n",
    "        return cnt\n",
    "                \n",
    "    # WIP\n",
    "    def custom_categorize(self):\n",
    "        df = pd.DataFrame(columns=['paper_obj', 'abstract', 'n_keywords'])\n",
    "        with db_session:\n",
    "            paper_list = select(p for p in Papers if p.abstract != None)[:]\n",
    "\n",
    "        overall_list = []\n",
    "        for paper in paper_list:\n",
    "            abst_words = self.nlp(paper.abstract.split(' '))\n",
    "            custom_keywords_raw = self.ngram_analyze(abst_words)\n",
    "            custom_keywords_pro = []\n",
    "            for c in custom_keywords_raw:\n",
    "                custom_keywords_pro += c\n",
    "            # lst = [paper, paper.abstract, ]\n",
    "\n",
    "    def howmany(self):\n",
    "        count = 0\n",
    "        for paper in self.data:\n",
    "            if paper.keywords != None:\n",
    "                count+=1 \n",
    "        print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_keywords(paper):\n",
    "    \"\"\"\n",
    "    Given a Paper object, this function will parse that paper's keywords and split those keywords into individual words.\n",
    "    \"\"\"\n",
    "    key_singular = []\n",
    "    if paper.keywords:\n",
    "        for i in paper.keywords.split(','):\n",
    "            split = i.split()\n",
    "            for j in split:\n",
    "                key_singular.append(j)\n",
    "    #else:\n",
    "        # generate your own keywords ~9\n",
    "        #WIP\n",
    "    return key_singular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create PaperAnalysis Object and categorize the keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = PaperAnalysis()\n",
    "cnt = x.categorize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the Glove word-to-vector txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.3 s, sys: 1.14 s, total: 26.5 s\n",
      "Wall time: 27.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models import KeyedVectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format('glove.6B.50d.cformat.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find how many keywords in the Alzheimer's subset are in this library of words-to-vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1565\n",
      "Percent of keywords in the glove file:  0.8733258928571429\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "all_  =0\n",
    "word_vex = []\n",
    "strings = []\n",
    "for key in cnt:\n",
    "    all_ += 1\n",
    "    if key in word_vectors:\n",
    "        word_vex.append(word_vectors[key])\n",
    "        strings.append(key)\n",
    "        count+=1\n",
    "print(len(word_vex))\n",
    "# accept anything over 85%\n",
    "print(\"Percent of keywords in the glove file: \", count/all_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster the word vectors using KMeans with 75 Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusters vectored words using kmeans with 75 clusters\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=75, random_state=0).fit(word_vex)\n",
    "#print(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cluster     string                                               word\n",
      "0       59     middle  [-0.15502, 1.0436, -0.22143, -0.89604, 0.49455...\n",
      "1       56   cerebral  [0.70895, 1.1627, -0.1386, -0.26542, -1.4248, ...\n",
      "2       72     artery  [1.545, 0.9012, 0.40715, -0.56387, -1.2987, 1....\n",
      "3       56  occlusion  [1.203, 0.030317, -0.43059, -0.62844, -1.2806,...\n",
      "4       14      blood  [0.88984, 0.35322, 0.067416, -0.87277, 0.29801...\n"
     ]
    }
   ],
   "source": [
    "# holds cluster number, actual word, and word vector\n",
    "d = {'word':word_vex, 'cluster':kmeans.labels_, 'string':strings}\n",
    "df = pd.DataFrame(data=d)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary associating words to clusters\n",
    "cluster_dict = dict()\n",
    "for i in range(df.min(axis=0).cluster, df.max(axis=0).cluster):\n",
    "    if i in cluster_dict:\n",
    "        cluster_dict[i].append(df[df['cluster'] == i].string)\n",
    "    else:\n",
    "        cluster_dict[i] = df[df['cluster'] == i].string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152             kinase\n",
       " 192        phosphatase\n",
       " 227     phosphorylated\n",
       " 380           tyrosine\n",
       " 525           cysteine\n",
       " 559             lysine\n",
       " 608             serine\n",
       " 877         methionine\n",
       " 1181       hydroxylase\n",
       " 1182             amino\n",
       " Name: string, dtype: object, 132       signal\n",
       " 170      channel\n",
       " 476      network\n",
       " 765    frequency\n",
       " 886     spectrum\n",
       " Name: string, dtype: object)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_dict[0], cluster_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Holds dataframe with list of papers and their corresponding topics (clusters)\n",
    "topic_paper = pd.DataFrame(columns=['topic', 'paper_list'])\n",
    "topic_paper['topic'] = list(range(75))\n",
    "topic_paper.head()\n",
    "topic_paper['paper_list'] = ''\n",
    "topic_paper['paper_list'] = topic_paper['paper_list'].astype(object)\n",
    "#topic_paper = topic_paper['paper_list'].apply(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill topic_paper with all papers corresponding to clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 36s, sys: 1.1 s, total: 1min 37s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# fills topic_paper dataframe with all papers belonging to a clusters\n",
    "for paper in x.data:\n",
    "    for keyword in parse_keywords(paper):\n",
    "        for word in strings:\n",
    "            if keyword == word:\n",
    "                cluster_no = df[df['string'] == keyword].cluster.item()\n",
    "                if topic_paper[topic_paper['topic'] == cluster_no].paper_list.item() == '':\n",
    "                    arr = [paper]\n",
    "                    topic_paper.set_value(cluster_no, 'paper_list',arr)\n",
    "                else:\n",
    "                    arr = topic_paper[topic_paper['topic'] == cluster_no].paper_list.item()\n",
    "                    arr.append(paper)\n",
    "                    topic_paper.set_value(cluster_no, 'paper_list',arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>paper_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[Papers[153049], Papers[380987], Papers[419914...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Papers[534915], Papers[1134081], Papers[11807...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[Papers[534915], Papers[1064996], Papers[11347...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[Papers[16934], Papers[137500], Papers[380987]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[Papers[544065], Papers[1087941], Papers[11291...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic                                         paper_list\n",
       "0      0  [Papers[153049], Papers[380987], Papers[419914...\n",
       "1      1  [Papers[534915], Papers[1134081], Papers[11807...\n",
       "2      2  [Papers[534915], Papers[1064996], Papers[11347...\n",
       "3      3  [Papers[16934], Papers[137500], Papers[380987]...\n",
       "4      4  [Papers[544065], Papers[1087941], Papers[11291..."
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_paper.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
